name: Push to Hugging Face Hub

on:
  push:
    branches: [ main ]
    tags: [ 'v*' ]
  workflow_dispatch:

jobs:
  push-metadata:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install huggingface-hub[cli] transformers
    
    - name: Configure Git
      run: |
        git config --global user.name "Oracle850B Bot"
        git config --global user.email "oracle850b@example.com"
    
    - name: Push metadata to HF Hub
      env:
        HUGGINGFACE_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
        HF_REPO: ${{ secrets.HF_REPO }}
        HF_TIER: ${{ secrets.HF_TIER || 'free' }}
      run: |
        echo "üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö Oracle850B –≤ Hugging Face Hub..."
        
        # –°–æ–∑–¥–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        mkdir -p hf_repo
        cd hf_repo
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å git —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
        git init
        
        # –î–æ–±–∞–≤–∏—Ç—å remote
        git remote add origin https://huggingface.co/$HF_REPO
        
        # –°–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        cp ../README.md .
        cp ../MODEL_CARD.md .
        cp ../configs/model/oracle850b.moe.json .
        cp -r ../checkpoints/oracle850b/tokenizer/ ./tokenizer/
        cp ../checkpoints/oracle850b/default_system.txt .
        cp ../checkpoints/oracle850b/default_intro.txt .
        
        # –°–æ–∑–¥–∞—Ç—å .gitattributes
        cat > .gitattributes << EOF
        *.json linguist-language=JSON
        *.txt linguist-language=Text
        *.md linguist-language=Markdown
        tokenizer/* linguist-vendored
        EOF
        
        # –°–æ–∑–¥–∞—Ç—å .gitignore
        cat > .gitignore << EOF
        # Model weights (too large for free tier)
        *.bin
        *.safetensors
        *.pt
        *.pth
        *.ckpt
        
        # Training artifacts
        logs/
        checkpoints/oracle850b/step-*/
        
        # Temporary files
        .DS_Store
        __pycache__/
        *.pyc
        EOF
        
        # –î–æ–±–∞–≤–∏—Ç—å —Ñ–∞–π–ª—ã
        git add .
        
        # –ö–æ–º–º–∏—Ç
        git commit -m "Add Oracle850B metadata and configs
        
        - Model configuration (MoE 850B parameters)
        - Tokenizer configuration
        - Default system and intro prompts
        - Model card and documentation
        
        Author: MagistrTheOne|–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä|2025"
        
        # Push –≤ HF Hub
        echo "üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Hugging Face Hub..."
        git push origin main
        
        echo "‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ HF Hub"
        echo "üí° –í–µ—Å–∞ –º–æ–¥–µ–ª–∏ –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ"
    
    - name: Create model card
      run: |
        echo "üìù –°–æ–∑–¥–∞–Ω–∏–µ MODEL_CARD.md..."
        
        cat > MODEL_CARD.md << 'EOF'
        # Oracle850B (MoE)
        
        ## Model Description
        
        Oracle850B is a Mixture of Experts (MoE) language model with approximately 850 billion parameters, developed by MagistrTheOne|–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä|2025.
        
        ### Architecture
        
        - **Model Type**: Decoder-only Transformer with MoE
        - **Total Parameters**: ~850B
        - **Active Parameters**: ~110-130B (top-k=2)
        - **Experts**: 64
        - **Context Length**: 8192 tokens
        - **Vocabulary Size**: 65536
        
        ### Special Tokens
        
        - `<|oracle_sys|>`: System token
        - `<|oracle_intro|>`: Introduction token
        - `<|author|>`: Author token
        - `<|endoftext|>`: End of text
        - `<|pad|>`: Padding
        - `<|unk|>`: Unknown
        
        ## Usage
        
        ```python
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        tokenizer = AutoTokenizer.from_pretrained("your-org/oracle850b")
        model = AutoModelForCausalLM.from_pretrained("your-org/oracle850b")
        
        # Generate text
        inputs = tokenizer("Hello, I am Oracle850B", return_tensors="pt")
        outputs = model.generate(**inputs, max_length=100)
        print(tokenizer.decode(outputs[0]))
        ```
        
        ## Training
        
        - **Framework**: Custom training pipeline
        - **Hardware**: 8x GPU cluster with TP/PP/SP
        - **Precision**: bf16
        - **Optimizer**: AdamW
        - **Learning Rate**: 1.2e-4
        
        ## Limitations
        
        - Model weights not yet available (training in progress)
        - Requires significant computational resources for inference
        - Optimized for Russian and English languages
        
        ## Citation
        
        ```bibtex
        @misc{oracle850b2025,
          title={Oracle850B: A Mixture of Experts Language Model},
          author={MagistrTheOne},
          year={2025},
          url={https://huggingface.co/your-org/oracle850b}
        }
        ```
        
        ## License
        
        [License to be determined]
        
        ## Contact
        
        - Author: MagistrTheOne|–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä|2025
        - Repository: https://github.com/MagistrTheOne/oracle850b
        EOF
        
        echo "‚úÖ MODEL_CARD.md —Å–æ–∑–¥–∞–Ω"
    
    - name: Update repository info
      env:
        HUGGINGFACE_TOKEN: ${{ secrets.HUGGINGFACE_TOKEN }}
        HF_REPO: ${{ secrets.HF_REPO }}
      run: |
        echo "üìù –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è..."
        
        # –û–±–Ω–æ–≤–∏—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        python -c "
        from huggingface_hub import HfApi
        api = HfApi(token='$HUGGINGFACE_TOKEN')
        api.update_repo_visibility(repo_id='$HF_REPO', private=False)
        print('‚úÖ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –æ–±–Ω–æ–≤–ª–µ–Ω')
        "
