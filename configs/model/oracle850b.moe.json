{
  "model_name": "oracle850b-moe",
  "arch": "decoder-only",
  "param_total": 850000000000,
  "moe": {
    "experts": 128,
    "expert_hidden": 2816,
    "router": {
      "type": "topk",
      "k": 2,
      "load_balancing_loss": 0.01
    }
  },
  "dense": {
    "d_model": 8192,
    "n_layers": 96,
    "n_heads": 64,
    "d_ff": 24576
  },
  "activation": "swiglu",
  "rope_theta": 10000,
  "rotary_pct": 0.5,
  "rmsnorm_eps": 1e-5,
  "flash_attn": true,
  "kv_cache": true,
  "vocab_size": 131072,
  "max_seq_len": 16384,
  "fp": {
    "train": "bf16",
    "infer": "auto"
  }
}
