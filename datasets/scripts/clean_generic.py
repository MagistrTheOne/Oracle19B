#!/usr/bin/env python3
"""
Oracle850B Data Cleaning
ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ, dedup, ÑÐ·Ñ‹Ðº, PII, Ñ‚Ð¾ÐºÑÐ¸Ñ‡Ð½Ð¾ÑÑ‚ÑŒ
Author: MagistrTheOne|ÐšÑ€Ð°ÑÐ½Ð¾Ð´Ð°Ñ€|2025
"""

import re
import json
import hashlib
import argparse
from pathlib import Path
from typing import List, Dict, Any, Set, Optional
from datetime import datetime
import unicodedata
from collections import defaultdict


class OracleDataCleaner:
    """ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Oracle850B"""
    
    def __init__(self, output_dir: str = "data/clean"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ÐŸÐ°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ð´Ð»Ñ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸
        self.pii_patterns = [
            r'\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b',  # ÐÐ¾Ð¼ÐµÑ€Ð° ÐºÐ°Ñ€Ñ‚
            r'\b\d{3}-\d{2}-\d{4}\b',  # SSN
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',  # Email
            r'\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b',  # IP Ð°Ð´Ñ€ÐµÑÐ°
        ]
        
        # Ð¢Ð¾ÐºÑÐ¸Ñ‡Ð½Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð° (ÑƒÐ¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº)
        self.toxic_words = {
            'hate', 'violence', 'abuse', 'harassment', 'discrimination'
        }
        
        # Ð¥ÐµÑˆÐ¸ Ð´Ð»Ñ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸
        self.content_hashes: Set[str] = set()
        self.minhash_hashes: Dict[str, Set[str]] = defaultdict(set)
    
    def normalize_text(self, text: str) -> str:
        """ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‚ÐµÐºÑÑ‚Ð°"""
        # Unicode Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
        text = unicodedata.normalize('NFC', text)
        
        # Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð»Ð¸ÑˆÐ½Ð¸Ñ… Ð¿Ñ€Ð¾Ð±ÐµÐ»Ð¾Ð²
        text = re.sub(r'\s+', ' ', text)
        
        # Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ ÑƒÐ¿Ñ€Ð°Ð²Ð»ÑÑŽÑ‰Ð¸Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)
        
        return text.strip()
    
    def detect_language(self, text: str) -> str:
        """ÐŸÑ€Ð¾ÑÑ‚Ð¾Ðµ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÑÐ·Ñ‹ÐºÐ° (ÑƒÐ¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ð¾Ðµ)"""
        # ÐŸÐ¾Ð´ÑÑ‡ÐµÑ‚ ÐºÐ¸Ñ€Ð¸Ð»Ð»Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¸ Ð»Ð°Ñ‚Ð¸Ð½ÑÐºÐ¸Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²
        cyrillic_count = len(re.findall(r'[Ð°-ÑÑ‘]', text.lower()))
        latin_count = len(re.findall(r'[a-z]', text.lower()))
        
        if cyrillic_count > latin_count:
            return 'ru'
        elif latin_count > 0:
            return 'en'
        else:
            return 'unknown'
    
    def detect_pii(self, text: str) -> List[str]:
        """ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ðµ PII"""
        pii_found = []
        
        for pattern in self.pii_patterns:
            matches = re.findall(pattern, text)
            if matches:
                pii_found.extend(matches)
        
        return pii_found
    
    def detect_toxicity(self, text: str) -> Dict[str, Any]:
        """ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ðµ Ñ‚Ð¾ÐºÑÐ¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸ (ÑƒÐ¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ð¾Ðµ)"""
        text_lower = text.lower()
        
        toxic_found = []
        for word in self.toxic_words:
            if word in text_lower:
                toxic_found.append(word)
        
        return {
            "is_toxic": len(toxic_found) > 0,
            "toxic_words": toxic_found,
            "toxicity_score": len(toxic_found) / len(text.split()) if text.split() else 0
        }
    
    def calculate_content_hash(self, text: str) -> str:
        """Ð’Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÑŒ Ñ…ÐµÑˆ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ð³Ð¾"""
        return hashlib.sha256(text.encode('utf-8')).hexdigest()
    
    def calculate_minhash(self, text: str, num_hashes: int = 64) -> Set[str]:
        """Ð’Ñ‹Ñ‡Ð¸ÑÐ»Ð¸Ñ‚ÑŒ MinHash Ð´Ð»Ñ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸"""
        # Ð£Ð¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ð°Ñ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ MinHash
        words = text.lower().split()
        shingles = set()
        
        # Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ ÑˆÐ¸Ð½Ð³Ð»Ñ‹
        for i in range(len(words) - 2):
            shingle = ' '.join(words[i:i+3])
            shingles.add(shingle)
        
        # Ð¥ÐµÑˆÐ¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÑˆÐ¸Ð½Ð³Ð»Ñ‹
        hashes = set()
        for shingle in shingles:
            hash_val = hash(shingle) % (2**32)
            hashes.add(str(hash_val))
        
        return hashes
    
    def is_duplicate(self, text: str, threshold: float = 0.8) -> bool:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð½Ð° Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚"""
        content_hash = self.calculate_content_hash(text)
        
        # Ð¢Ð¾Ñ‡Ð½Ð¾Ðµ ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ðµ
        if content_hash in self.content_hashes:
            return True
        
        # MinHash ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸Ðµ
        text_minhash = self.calculate_minhash(text)
        
        for existing_minhash in self.minhash_hashes.values():
            intersection = len(text_minhash & existing_minhash)
            union = len(text_minhash | existing_minhash)
            
            if union > 0 and intersection / union >= threshold:
                return True
        
        return False
    
    def clean_text(self, text: str) -> Dict[str, Any]:
        """ÐžÑ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑÑ‚"""
        
        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
        normalized_text = self.normalize_text(text)
        
        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÑÐ·Ñ‹ÐºÐ°
        language = self.detect_language(normalized_text)
        
        # ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ðµ PII
        pii_found = self.detect_pii(normalized_text)
        
        # ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð¸Ðµ Ñ‚Ð¾ÐºÑÐ¸Ñ‡Ð½Ð¾ÑÑ‚Ð¸
        toxicity = self.detect_toxicity(normalized_text)
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚
        is_dup = self.is_duplicate(normalized_text)
        
        # Ð ÐµÑˆÐµÐ½Ð¸Ðµ Ð¾ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ð¸
        should_keep = (
            len(normalized_text) >= 50 and  # ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ Ð´Ð»Ð¸Ð½Ð°
            language in ['ru', 'en'] and  # ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÐ¼Ñ‹Ðµ ÑÐ·Ñ‹ÐºÐ¸
            len(pii_found) == 0 and  # ÐÐµÑ‚ PII
            not toxicity['is_toxic'] and  # ÐÐµ Ñ‚Ð¾ÐºÑÐ¸Ñ‡Ð½Ð¾
            not is_dup  # ÐÐµ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚
        )
        
        result = {
            "original_text": text,
            "cleaned_text": normalized_text,
            "language": language,
            "pii_found": pii_found,
            "toxicity": toxicity,
            "is_duplicate": is_dup,
            "should_keep": should_keep,
            "length": len(normalized_text),
            "cleaned_at": datetime.now().isoformat()
        }
        
        # ÐžÐ±Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ…ÐµÑˆÐ¸ ÐµÑÐ»Ð¸ Ñ‚ÐµÐºÑÑ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ÑÑ
        if should_keep:
            content_hash = self.calculate_content_hash(normalized_text)
            self.content_hashes.add(content_hash)
            
            minhash = self.calculate_minhash(normalized_text)
            self.minhash_hashes[content_hash] = minhash
        
        return result
    
    def clean_file(self, input_file: Path, output_file: Path = None) -> Dict[str, Any]:
        """ÐžÑ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ Ñ„Ð°Ð¹Ð»"""
        
        if output_file is None:
            output_file = self.output_dir / f"cleaned_{input_file.name}"
        
        print(f"ðŸ”„ ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ñ„Ð°Ð¹Ð»Ð°: {input_file}")
        
        cleaned_texts = []
        stats = {
            "total_lines": 0,
            "kept_lines": 0,
            "removed_duplicates": 0,
            "removed_pii": 0,
            "removed_toxic": 0,
            "removed_short": 0,
            "removed_wrong_lang": 0
        }
        
        with open(input_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                stats["total_lines"] += 1
                
                if line_num % 1000 == 0:
                    print(f"  ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ ÑÑ‚Ñ€Ð¾Ðº: {line_num}")
                
                # ÐžÑ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ ÑÑ‚Ñ€Ð¾ÐºÑƒ
                result = self.clean_text(line.strip())
                
                if result["should_keep"]:
                    cleaned_texts.append(result["cleaned_text"])
                    stats["kept_lines"] += 1
                else:
                    # ÐŸÐ¾Ð´ÑÑ‡ÐµÑ‚ Ð¿Ñ€Ð¸Ñ‡Ð¸Ð½ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ
                    if result["is_duplicate"]:
                        stats["removed_duplicates"] += 1
                    if len(result["pii_found"]) > 0:
                        stats["removed_pii"] += 1
                    if result["toxicity"]["is_toxic"]:
                        stats["removed_toxic"] += 1
                    if result["length"] < 50:
                        stats["removed_short"] += 1
                    if result["language"] not in ['ru', 'en']:
                        stats["removed_wrong_lang"] += 1
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ð½Ñ‹Ðµ Ñ‚ÐµÐºÑÑ‚Ñ‹
        with open(output_file, 'w', encoding='utf-8') as f:
            for text in cleaned_texts:
                f.write(text + '\n')
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ
        stats_file = output_file.with_suffix('.stats.json')
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°: {stats['kept_lines']}/{stats['total_lines']} ÑÑ‚Ñ€Ð¾Ðº ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾")
        print(f"ðŸ“Š Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°: {json.dumps(stats, ensure_ascii=False)}")
        
        return stats


def main():
    parser = argparse.ArgumentParser(description="Oracle850B Data Cleaning")
    parser.add_argument("--input-file", required=True, help="Ð’Ñ…Ð¾Ð´Ð½Ð¾Ð¹ Ñ„Ð°Ð¹Ð»")
    parser.add_argument("--output-file", help="Ð’Ñ‹Ñ…Ð¾Ð´Ð½Ð¾Ð¹ Ñ„Ð°Ð¹Ð»")
    parser.add_argument("--output-dir", default="data/clean", help="Ð’Ñ‹Ñ…Ð¾Ð´Ð½Ð°Ñ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ñ")
    
    args = parser.parse_args()
    
    cleaner = OracleDataCleaner(args.output_dir)
    
    input_path = Path(args.input_file)
    output_path = Path(args.output_file) if args.output_file else None
    
    stats = cleaner.clean_file(input_path, output_path)
    
    print(f"âœ… ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°: {stats['kept_lines']}/{stats['total_lines']} ÑÑ‚Ñ€Ð¾Ðº")


if __name__ == "__main__":
    main()
