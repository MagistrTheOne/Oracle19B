#!/usr/bin/env python3
"""
Oracle850B Parquet Export
–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤ Parquet —Ñ–æ—Ä–º–∞—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
Author: MagistrTheOne|–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä|2025
"""

import json
import argparse
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
import pandas as pd


class OracleParquetExporter:
    """–≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö Oracle850B –≤ Parquet"""
    
    def __init__(self, output_dir: str = "data/parquet"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def export_file_to_parquet(self, input_file: Path, output_file: Path = None,
                              batch_size: int = 10000) -> Path:
        """–≠–∫—Å–ø–æ—Ä—Ç —Ñ–∞–π–ª–∞ –≤ Parquet"""
        
        if output_file is None:
            output_file = self.output_dir / f"{input_file.stem}.parquet"
        
        print(f"üîÑ –≠–∫—Å–ø–æ—Ä—Ç –≤ Parquet: {input_file}")
        print(f"üìä –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
        
        # –ß–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –±–∞—Ç—á–∞–º–∏
        data_batches = []
        batch_data = []
        
        with open(input_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                text = line.strip()
                if not text:
                    continue
                
                # –î–æ–±–∞–≤–∏—Ç—å –≤ –±–∞—Ç—á
                batch_data.append({
                    "text": text,
                    "text_length": len(text),
                    "line_number": line_num,
                    "source_file": str(input_file),
                    "exported_at": datetime.now().isoformat()
                })
                
                # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –±–∞—Ç—á –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç —Ä–∞–∑–º–µ—Ä
                if len(batch_data) >= batch_size:
                    data_batches.append(batch_data.copy())
                    batch_data = []
                    
                    if line_num % 100000 == 0:
                        print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫: {line_num}")
        
        # –î–æ–±–∞–≤–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á
        if batch_data:
            data_batches.append(batch_data)
        
        # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤—Å–µ –±–∞—Ç—á–∏
        all_data = []
        for batch in data_batches:
            all_data.extend(batch)
        
        # –°–æ–∑–¥–∞—Ç—å DataFrame
        df = pd.DataFrame(all_data)
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ Parquet
        df.to_parquet(output_file, index=False, compression='snappy')
        
        print(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à—ë–Ω: {output_file}")
        print(f"üìä –°—Ç—Ä–æ–∫ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(df)}")
        
        return output_file
    
    def export_webdataset_to_parquet(self, webdataset_dir: Path, 
                                    split: str = "train") -> Path:
        """–≠–∫—Å–ø–æ—Ä—Ç WebDataset –≤ Parquet"""
        
        print(f"üîÑ –≠–∫—Å–ø–æ—Ä—Ç WebDataset –≤ Parquet: {webdataset_dir}")
        
        # –ù–∞–π—Ç–∏ –≤—Å–µ —à–∞—Ä–¥—ã
        split_dir = webdataset_dir / split
        shard_files = list(split_dir.glob("shard-*.tar"))
        
        if not shard_files:
            print(f"‚ùå –®–∞—Ä–¥—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {split_dir}")
            return None
        
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ —à–∞—Ä–¥–æ–≤: {len(shard_files)}")
        
        # –ò–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —à–∞—Ä–¥–æ–≤
        all_data = []
        
        for shard_file in shard_files:
            print(f"  –û–±—Ä–∞–±–æ—Ç–∫–∞ —à–∞—Ä–¥–∞: {shard_file.name}")
            
            # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç—ã –∏–∑ tar
            shard_data = self._extract_shard_data(shard_file)
            all_data.extend(shard_data)
        
        # –°–æ–∑–¥–∞—Ç—å DataFrame
        df = pd.DataFrame(all_data)
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ Parquet
        output_file = self.output_dir / f"{split}.parquet"
        df.to_parquet(output_file, index=False, compression='snappy')
        
        print(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à—ë–Ω: {output_file}")
        print(f"üìä –°—Ç—Ä–æ–∫ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(df)}")
        
        return output_file
    
    def _extract_shard_data(self, shard_file: Path) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ —à–∞—Ä–¥–∞"""
        
        import tarfile
        
        shard_data = []
        
        with tarfile.open(shard_file, 'r') as tar:
            for member in tar.getmembers():
                if member.isfile() and member.name.endswith('.txt'):
                    # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç
                    file_obj = tar.extractfile(member)
                    if file_obj:
                        text = file_obj.read().decode('utf-8')
                        
                        shard_data.append({
                            "text": text,
                            "text_length": len(text),
                            "shard_file": str(shard_file),
                            "file_in_shard": member.name,
                            "extracted_at": datetime.now().isoformat()
                        })
        
        return shard_data
    
    def create_parquet_manifest(self, parquet_files: List[Path]) -> Path:
        """–°–æ–∑–¥–∞—Ç—å –º–∞–Ω–∏—Ñ–µ—Å—Ç Parquet —Ñ–∞–π–ª–æ–≤"""
        
        manifest_path = self.output_dir / "parquet_manifest.json"
        
        manifest = {
            "total_files": len(parquet_files),
            "created_at": datetime.now().isoformat(),
            "files": []
        }
        
        for parquet_file in parquet_files:
            file_info = {
                "file_path": str(parquet_file.relative_to(self.output_dir)),
                "file_size_bytes": parquet_file.stat().st_size,
                "file_size_mb": parquet_file.stat().st_size / (1024 * 1024)
            }
            
            # –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –ø–æ–ª—É—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
            try:
                df = pd.read_parquet(parquet_file)
                file_info["total_rows"] = len(df)
            except Exception as e:
                file_info["error"] = str(e)
            
            manifest["files"].append(file_info)
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–∞–Ω–∏—Ñ–µ—Å—Ç
        with open(manifest_path, 'w', encoding='utf-8') as f:
            json.dump(manifest, f, indent=2, ensure_ascii=False)
        
        print(f"üìã –ú–∞–Ω–∏—Ñ–µ—Å—Ç Parquet —Å–æ–∑–¥–∞–Ω: {manifest_path}")
        return manifest_path


def main():
    parser = argparse.ArgumentParser(description="Oracle850B Parquet Export")
    parser.add_argument("--input-file", help="–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª")
    parser.add_argument("--webdataset-dir", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è WebDataset")
    parser.add_argument("--split", default="train", help="Split –¥–ª—è WebDataset")
    parser.add_argument("--output-dir", default="data/parquet", help="–í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è")
    parser.add_argument("--batch-size", type=int, default=10000, help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞")
    
    args = parser.parse_args()
    
    exporter = OracleParquetExporter(args.output_dir)
    
    if args.input_file:
        # –≠–∫—Å–ø–æ—Ä—Ç —Ñ–∞–π–ª–∞
        input_path = Path(args.input_file)
        output_path = exporter.export_file_to_parquet(
            input_path, batch_size=args.batch_size
        )
        print(f"‚úÖ –§–∞–π–ª —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: {output_path}")
        
    elif args.webdataset_dir:
        # –≠–∫—Å–ø–æ—Ä—Ç WebDataset
        webdataset_path = Path(args.webdataset_dir)
        output_path = exporter.export_webdataset_to_parquet(
            webdataset_path, args.split
        )
        if output_path:
            print(f"‚úÖ WebDataset —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: {output_path}")
    else:
        print("‚ùå –£–∫–∞–∂–∏—Ç–µ --input-file –∏–ª–∏ --webdataset-dir")


if __name__ == "__main__":
    main()
