#!/usr/bin/env python3
"""
Oracle850B Data Statistics
–°–≤–æ–¥–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö, –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, —è–∑—ã–∫–æ–≤, —Ç–µ–º–∞—Ç–∏–∫
Author: MagistrTheOne|–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä|2025
"""

import json
import argparse
from pathlib import Path
from typing import List, Dict, Any, Counter
from datetime import datetime
from collections import defaultdict
import re


class OracleDataStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö Oracle850B"""
    
    def __init__(self, output_dir: str = "data/stats"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def analyze_file(self, input_file: Path) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞ –¥–∞–Ω–Ω—ã—Ö"""
        
        print(f"üîÑ –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {input_file}")
        
        stats = {
            "file_info": {
                "file_path": str(input_file),
                "file_size_bytes": input_file.stat().st_size,
                "analyzed_at": datetime.now().isoformat()
            },
            "text_stats": {
                "total_lines": 0,
                "non_empty_lines": 0,
                "total_characters": 0,
                "total_words": 0,
                "total_tokens": 0
            },
            "length_distribution": {
                "min_length": float('inf'),
                "max_length": 0,
                "avg_length": 0,
                "length_buckets": {}
            },
            "language_distribution": {},
            "quality_metrics": {
                "avg_words_per_line": 0,
                "avg_chars_per_line": 0,
                "lines_with_punctuation": 0,
                "lines_with_numbers": 0,
                "lines_with_uppercase": 0
            },
            "duplicate_analysis": {
                "exact_duplicates": 0,
                "near_duplicates": 0,
                "duplicate_rate": 0.0
            }
        }
        
        line_lengths = []
        languages = Counter()
        content_hashes = set()
        duplicate_hashes = set()
        
        with open(input_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                stats["text_stats"]["total_lines"] += 1
                
                if line_num % 100000 == 0:
                    print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫: {line_num}")
                
                line = line.strip()
                if not line:
                    continue
                
                stats["text_stats"]["non_empty_lines"] += 1
                
                # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                line_length = len(line)
                line_lengths.append(line_length)
                
                stats["text_stats"]["total_characters"] += line_length
                stats["text_stats"]["total_words"] += len(line.split())
                
                # –û–±–Ω–æ–≤–∏—Ç—å min/max –¥–ª–∏–Ω—ã
                if line_length < stats["length_distribution"]["min_length"]:
                    stats["length_distribution"]["min_length"] = line_length
                if line_length > stats["length_distribution"]["max_length"]:
                    stats["length_distribution"]["max_length"] = line_length
                
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
                language = self._detect_language(line)
                languages[language] += 1
                
                # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                if self._has_punctuation(line):
                    stats["quality_metrics"]["lines_with_punctuation"] += 1
                if self._has_numbers(line):
                    stats["quality_metrics"]["lines_with_numbers"] += 1
                if self._has_uppercase(line):
                    stats["quality_metrics"]["lines_with_uppercase"] += 1
                
                # –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                content_hash = hash(line)
                if content_hash in content_hashes:
                    duplicate_hashes.add(content_hash)
                    stats["duplicate_analysis"]["exact_duplicates"] += 1
                else:
                    content_hashes.add(content_hash)
        
        # –í—ã—á–∏—Å–ª–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if stats["text_stats"]["non_empty_lines"] > 0:
            stats["length_distribution"]["avg_length"] = sum(line_lengths) / len(line_lengths)
            stats["quality_metrics"]["avg_words_per_line"] = (
                stats["text_stats"]["total_words"] / stats["text_stats"]["non_empty_lines"]
            )
            stats["quality_metrics"]["avg_chars_per_line"] = (
                stats["text_stats"]["total_characters"] / stats["text_stats"]["non_empty_lines"]
            )
            
            stats["duplicate_analysis"]["duplicate_rate"] = (
                stats["duplicate_analysis"]["exact_duplicates"] / stats["text_stats"]["non_empty_lines"]
            )
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤
        stats["language_distribution"] = dict(languages.most_common())
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω –ø–æ –∫–æ—Ä–∑–∏–Ω–∞–º
        stats["length_distribution"]["length_buckets"] = self._create_length_buckets(line_lengths)
        
        return stats
    
    def _detect_language(self, text: str) -> str:
        """–ü—Ä–æ—Å—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞"""
        cyrillic_count = len(re.findall(r'[–∞-—è—ë]', text.lower()))
        latin_count = len(re.findall(r'[a-z]', text.lower()))
        
        if cyrillic_count > latin_count:
            return 'ru'
        elif latin_count > 0:
            return 'en'
        else:
            return 'unknown'
    
    def _has_punctuation(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏"""
        return bool(re.search(r'[.!?,:;]', text))
    
    def _has_numbers(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —á–∏—Å–µ–ª"""
        return bool(re.search(r'\d', text))
    
    def _has_uppercase(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤"""
        return bool(re.search(r'[A-Z]', text))
    
    def _create_length_buckets(self, lengths: List[int]) -> Dict[str, int]:
        """–°–æ–∑–¥–∞—Ç—å –∫–æ—Ä–∑–∏–Ω—ã –¥–ª–∏–Ω"""
        buckets = {
            "0-50": 0,
            "51-100": 0,
            "101-200": 0,
            "201-500": 0,
            "501-1000": 0,
            "1000+": 0
        }
        
        for length in lengths:
            if length <= 50:
                buckets["0-50"] += 1
            elif length <= 100:
                buckets["51-100"] += 1
            elif length <= 200:
                buckets["101-200"] += 1
            elif length <= 500:
                buckets["201-500"] += 1
            elif length <= 1000:
                buckets["501-1000"] += 1
            else:
                buckets["1000+"] += 1
        
        return buckets
    
    def analyze_webdataset(self, webdataset_dir: Path, split: str = "train") -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ WebDataset"""
        
        print(f"üîÑ –ê–Ω–∞–ª–∏–∑ WebDataset: {webdataset_dir}/{split}")
        
        split_dir = webdataset_dir / split
        shard_files = list(split_dir.glob("shard-*.tar"))
        
        if not shard_files:
            return {"error": f"–®–∞—Ä–¥—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {split_dir}"}
        
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ —à–∞—Ä–¥–æ–≤: {len(shard_files)}")
        
        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_stats = {
            "webdataset_info": {
                "split": split,
                "total_shards": len(shard_files),
                "analyzed_at": datetime.now().isoformat()
            },
            "aggregated_stats": {
                "total_samples": 0,
                "total_size_bytes": 0,
                "avg_shard_size_mb": 0
            },
            "shard_details": []
        }
        
        total_samples = 0
        total_size = 0
        
        for shard_file in shard_files:
            print(f"  –ê–Ω–∞–ª–∏–∑ —à–∞—Ä–¥–∞: {shard_file.name}")
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —à–∞—Ä–¥–µ
            shard_size = shard_file.stat().st_size
            total_size += shard_size
            
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–Ω–¥–µ–∫—Å —à–∞—Ä–¥–∞
            idx_path = shard_file.with_suffix('.idx')
            shard_info = {}
            if idx_path.exists():
                with open(idx_path, 'r', encoding='utf-8') as f:
                    shard_info = json.load(f)
                    total_samples += shard_info.get("shard_info", {}).get("total_samples", 0)
            
            shard_detail = {
                "shard_file": str(shard_file.relative_to(webdataset_dir)),
                "shard_size_bytes": shard_size,
                "shard_size_mb": shard_size / (1024 * 1024),
                "total_samples": shard_info.get("shard_info", {}).get("total_samples", 0),
                "index_info": shard_info
            }
            
            total_stats["shard_details"].append(shard_detail)
        
        # –û–±–Ω–æ–≤–∏—Ç—å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        total_stats["aggregated_stats"]["total_samples"] = total_samples
        total_stats["aggregated_stats"]["total_size_bytes"] = total_size
        if len(shard_files) > 0:
            total_stats["aggregated_stats"]["avg_shard_size_mb"] = total_size / len(shard_files) / (1024 * 1024)
        
        return total_stats
    
    def generate_quality_report(self, stats: Dict[str, Any]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ"""
        
        report = {
            "quality_assessment": {
                "overall_score": 0.0,
                "issues": [],
                "recommendations": []
            },
            "data_health": {},
            "generated_at": datetime.now().isoformat()
        }
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫
        score = 1.0
        issues = []
        recommendations = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        duplicate_rate = stats.get("duplicate_analysis", {}).get("duplicate_rate", 0)
        if duplicate_rate > 0.1:
            score -= 0.2
            issues.append(f"–í—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicate_rate:.2%}")
            recommendations.append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —è–∑—ã–∫–æ–≤–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        lang_dist = stats.get("language_distribution", {})
        if len(lang_dist) == 1 and "unknown" in lang_dist:
            score -= 0.3
            issues.append("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤")
            recommendations.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤
        avg_length = stats.get("length_distribution", {}).get("avg_length", 0)
        if avg_length < 50:
            score -= 0.1
            issues.append(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–∞: {avg_length:.1f}")
            recommendations.append("–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        total_lines = stats.get("text_stats", {}).get("non_empty_lines", 0)
        punct_lines = stats.get("quality_metrics", {}).get("lines_with_punctuation", 0)
        if total_lines > 0 and punct_lines / total_lines < 0.5:
            score -= 0.1
            issues.append("–ù–∏–∑–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–∞—Ö")
            recommendations.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        
        # –û–±–Ω–æ–≤–∏—Ç—å –æ—Ç—á—ë—Ç
        report["quality_assessment"]["overall_score"] = max(0.0, score)
        report["quality_assessment"]["issues"] = issues
        report["quality_assessment"]["recommendations"] = recommendations
        
        # –û—Ü–µ–Ω–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è –¥–∞–Ω–Ω—ã—Ö
        report["data_health"] = {
            "duplicate_rate": duplicate_rate,
            "language_diversity": len(lang_dist),
            "avg_text_length": avg_length,
            "total_samples": stats.get("text_stats", {}).get("non_empty_lines", 0)
        }
        
        return report
    
    def save_stats(self, stats: Dict[str, Any], output_file: Path = None) -> Path:
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = self.output_dir / f"stats_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_file}")
        return output_file


def main():
    parser = argparse.ArgumentParser(description="Oracle850B Data Statistics")
    parser.add_argument("--input-file", help="–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    parser.add_argument("--webdataset-dir", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è WebDataset")
    parser.add_argument("--split", default="train", help="Split –¥–ª—è WebDataset")
    parser.add_argument("--output-dir", default="data/stats", help="–í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è")
    parser.add_argument("--quality-report", action="store_true", help="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ")
    
    args = parser.parse_args()
    
    stats_analyzer = OracleDataStats(args.output_dir)
    
    if args.input_file:
        # –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞
        input_path = Path(args.input_file)
        stats = stats_analyzer.analyze_file(input_path)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ
        if args.quality_report:
            quality_report = stats_analyzer.generate_quality_report(stats)
            stats["quality_report"] = quality_report
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        output_file = stats_analyzer.save_stats(stats)
        print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω: {output_file}")
        
    elif args.webdataset_dir:
        # –ê–Ω–∞–ª–∏–∑ WebDataset
        webdataset_path = Path(args.webdataset_dir)
        stats = stats_analyzer.analyze_webdataset(webdataset_path, args.split)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        output_file = stats_analyzer.save_stats(stats)
        print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ WebDataset –∑–∞–≤–µ—Ä—à—ë–Ω: {output_file}")
    else:
        print("‚ùå –£–∫–∞–∂–∏—Ç–µ --input-file –∏–ª–∏ --webdataset-dir")


if __name__ == "__main__":
    main()
